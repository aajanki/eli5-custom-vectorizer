{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eli5 text highlighting with a custom vectorizer\n",
    "\n",
    "This notebook shows how to do highlighted text explanation with eli5 library when using a custom vectorizer. This is necessary, for example, when vectorizer lemmatizes Finnish text with libvoikko."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import eli5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from eli5.base import DocWeightedSpans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler, LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from voikko import libvoikko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoikkoVectorizer\n",
    "\n",
    "The following cell implementes a custom scikit-learn Vectorizer that 1) uses libvoikko for lemmatization and 2) implements the eli5 interface required for text highlighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoikkoVectorizer(TfidfVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    \n",
    "    Based on the scikit-learn's TfidfVectorizer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    voikko : default = None\n",
    "        An instance of libvoikko.Voikko object. If given, the words\n",
    "        will be lemmatized using libvoikko.\n",
    "\n",
    "    Other parameters are the same as in TfidfVectorizer (except for\n",
    "    tokenizer and analyzer which this class overrides).\n",
    "    \"\"\"\n",
    "    def __init__(self, *, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, stop_words=None,\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False, voikko=None):\n",
    "        self.voikko = voikko\n",
    "\n",
    "        if stop_words:\n",
    "            stop_words = set(self._simple_tokenizer(' '.join(stop_words)))\n",
    "\n",
    "        super().__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=self._simple_tokenizer,\n",
    "            analyzer='word', stop_words=stop_words, token_pattern=None, \n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype, norm=norm, use_idf=use_idf, smooth_idf=smooth_idf,\n",
    "            sublinear_tf=sublinear_tf)\n",
    "\n",
    "    def get_doc_weighted_spans(self, doc, feature_weights, feature_fn):\n",
    "        \"\"\"This function implements eli5's interface required for highlighted text.\n",
    "        \n",
    "        Adapted from eli5.sklearn.text.\"\"\"\n",
    "        preprocessed_doc = self.build_preprocessor()(self.decode(doc))\n",
    "        feature_weights_dict = _get_feature_weights_dict(feature_weights, feature_fn)\n",
    "        \n",
    "        spans = []\n",
    "        found_features = {}\n",
    "        for f_spans, feature in self._span_analyzer(preprocessed_doc):\n",
    "            if feature not in feature_weights_dict:\n",
    "                continue\n",
    "\n",
    "            weight, key = feature_weights_dict[feature]\n",
    "            spans.append((feature, f_spans, weight))\n",
    "            found_features[key] = weight\n",
    "\n",
    "        return found_features, DocWeightedSpans(\n",
    "            document=preprocessed_doc,\n",
    "            spans=spans,\n",
    "            preserve_density=self.analyzer.startswith('char'),\n",
    "        )\n",
    "\n",
    "    def _span_tokenizer(self, doc):\n",
    "        tokens = []\n",
    "        for m in re.finditer(r'\\b\\w\\w+\\b', doc):\n",
    "            token = m.group()\n",
    "            if self.voikko is not None:\n",
    "                analyzed = self.voikko.analyze(token)\n",
    "                if analyzed:\n",
    "                    token = analyzed[0].get('BASEFORM', token)\n",
    "\n",
    "            tokens.append((m.span(), token.lower()))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _simple_tokenizer(self, doc):\n",
    "        return [token for _, token in self._span_tokenizer(doc)]\n",
    "\n",
    "    def _span_analyzer(self, doc):\n",
    "        assert self.analyzer == 'word'\n",
    "        \n",
    "        tokens = self._span_tokenizer(doc)        \n",
    "        return self._span_word_ngrams(tokens)\n",
    "            \n",
    "    def _span_word_ngrams(self, tokens):\n",
    "        if self.stop_words is not None:\n",
    "            tokens = [(s, w) for s, w in tokens if w not in self.stop_words]\n",
    "\n",
    "        min_n, max_n = self.ngram_range\n",
    "        if max_n == 1:\n",
    "            tokens = [([s], w) for s, w in tokens]\n",
    "        else:\n",
    "            original_tokens = tokens\n",
    "            tokens = []\n",
    "            n_original_tokens = len(original_tokens)\n",
    "            tokens_append = tokens.append\n",
    "            space_join = ' '.join\n",
    "            for n in range(min_n,\n",
    "                            min(max_n + 1, n_original_tokens + 1)):\n",
    "                for i in range(n_original_tokens - n + 1):\n",
    "                    ngram_tokens = original_tokens[i: i + n]\n",
    "                    tokens_append((\n",
    "                        [s for s, _ in ngram_tokens],\n",
    "                        space_join(t for _, t in ngram_tokens)))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "def _get_feature_weights_dict(feature_weights,  # type: FeatureWeights\n",
    "                              feature_fn        # type: Optional[Callable[[str], str]]\n",
    "                              ):\n",
    "    # type: (...) -> Dict[str, Tuple[float, Tuple[str, int]]]\n",
    "    \"\"\" Return {feat_name: (weight, (group, idx))} mapping.\n",
    "    \n",
    "    Copied from eli5.sklearn.text.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # (group, idx) is an unique feature identifier, e.g. ('pos', 2)\n",
    "        feat_name: (fw.weight, (group, idx))\n",
    "        for group in ['pos', 'neg']\n",
    "        for idx, fw in enumerate(getattr(feature_weights, group))\n",
    "        for feat_name in _get_features(fw.feature, feature_fn)\n",
    "    }\n",
    "\n",
    "def _get_features(feature, feature_fn=None):\n",
    "    \"\"\"Copied from eli5.sklearn.text.\"\"\"\n",
    "    if isinstance(feature, list):\n",
    "        features = [f['name'] for f in feature]\n",
    "    else:\n",
    "        features = [feature]\n",
    "    if feature_fn:\n",
    "        features = list(filter(None, map(feature_fn, features)))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_classes = [\n",
    "    'ulkomaankauppa- ja kehitysministeri',\n",
    "    'puolustusministeri',\n",
    "    'pääministeri',\n",
    "    'eurooppa-, kulttuuri- ja urheiluministeri'\n",
    "]\n",
    "\n",
    "short_names = {\n",
    "    'perhe- ja peruspalveluministeri': 'per',\n",
    "    'maatalous- ja ympäristöministeri': 'maa',\n",
    "    'sisäministeri': 'sis',\n",
    "    'oikeus- ja työministeri': 'oik',\n",
    "    'opetus- ja kulttuuriministeri': 'ope',\n",
    "    'valtiovarainministeri': 'val',\n",
    "    'liikenne- ja viestintäministeri': 'lii',\n",
    "    'sosiaali- ja terveysministeri': 'sos',\n",
    "    'elinkeinoministeri': 'eli',\n",
    "    'ulkoministeri': 'ulk',\n",
    "    'kunta- ja uudistusministeri': 'kun',\n",
    "    'eurooppa-, kulttuuri- ja urheiluministeri': 'eur',\n",
    "    'pääministeri': 'pää',\n",
    "    'puolustusministeri': 'puo',\n",
    "    'ulkomaankauppa- ja kehitysministeri': 'uke',\n",
    "}\n",
    "\n",
    "def load_documents(filename):\n",
    "    df = pd.read_csv(filename, header=0).rename(columns={'ministry': 'class'})\n",
    "    df = df[~df['class'].isin(small_classes)].reset_index()\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "    train = load_documents('data/vkk/train.csv.bz2')\n",
    "    dev = load_documents('data/vkk/dev.csv.bz2')\n",
    "    test = load_documents('data/vkk/test.csv.bz2')\n",
    "    \n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = load_data()\n",
    "print(f'Number of classes: {len(train[\"class\"].unique())}')\n",
    "print(f'Number of train samples: {len(train)}')\n",
    "print(f'Number of dev samples: {len(dev)}')\n",
    "print(f'Number of test samples: {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voikko = libvoikko.Voikko('fi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_fi = [\n",
    "    'ei', 'että', 'he', 'hän', 'ja', 'joissa', 'joka', 'jos', 'koska', 'kuin',\n",
    "    'kuka', 'kun', 'me', 'mikä', 'minä', 'myös', 'ne', 'nuo', 'nämä', 'olla',\n",
    "    'se', 'sinä', 'tai', 'te', 'tuo', 'tämä', 'vai',\n",
    "]\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y_encoded = enc.fit_transform(train['class'])\n",
    "\n",
    "vec = VoikkoVectorizer(voikko=voikko, \n",
    "                       ngram_range=(1, 2),\n",
    "                       min_df=2, max_df=0.1,\n",
    "                       stop_words=stop_words_fi)\n",
    "\n",
    "clf = LinearSVC(C=0.1, loss='hinge', intercept_scaling=5.0,\n",
    "                max_iter=100000, multi_class='ovr')\n",
    "scaler = MaxAbsScaler()\n",
    "pipe = make_pipeline(vec, scaler, clf)\n",
    "pipe.fit(train['sentence'], y_encoded);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that the vectorizer lemmatizes Finnish words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Ajoimme punaisella autolla aamulla.'\n",
    "\n",
    "' '.join(vec.inverse_transform(vec.transform([word]))[0][0] for word in text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on the development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_true = dev['class']\n",
    "y_dev_pred = enc.inverse_transform(pipe.predict(dev['sentence']))\n",
    "\n",
    "print(classification_report(y_dev_true, y_dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the classifier and predictions\n",
    "\n",
    "First, show the top features for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = enc.inverse_transform(clf.classes_)\n",
    "\n",
    "eli5.show_weights(clf, vec=vec, top=10, target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, explain predictions on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = dev['sentence'].iloc[111]\n",
    "y_test = dev['class'].iloc[111]\n",
    "\n",
    "eli5.show_prediction(clf, doc_test, vec=vec, target_names=target_names, targets=[y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
